analyze and memorize my backend files don't explain just analyze


1. # server/run.py
import os
import sys
import logging
from app import create_app

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def create_application():
    """Create and configure the Flask application"""
    try:
        logger.info("üöÄ Starting Savlink backend...")
        
        # Create the Flask app
        app = create_app()
        
        # Log configuration info
        env = app.config.get('FLASK_ENV', 'production')
        debug = app.config.get('DEBUG', False)
        
        logger.info(f"‚úÖ Application created successfully")
        logger.info(f"üìù Environment: {env}")
        logger.info(f"üîß Debug mode: {debug}")
        
        return app
        
    except Exception as e:
        logger.error(f"‚ùå Failed to create application: {e}")
        sys.exit(1)

def main():
    """Main application entry point"""
    # Create the application
    app = create_application()
    
    # Get configuration - CRITICAL FIX FOR RENDER
    port = int(os.environ.get('PORT', 10000))  # Render uses PORT env var
    host = '0.0.0.0'  # Must be 0.0.0.0 for Render
    debug = app.config.get('DEBUG', False)
    env = app.config.get('FLASK_ENV', 'production')
    
    # Production check
    if env == 'production':
        logger.info("üöÄ Production mode - using Gunicorn is recommended")
    
    try:
        logger.info(f"üåê Starting server on http://{host}:{port}")
        logger.info(f"üìä Health check available at: http://{host}:{port}/health")
        
        # CRITICAL: Add immediate health check endpoint binding
        @app.route('/health')
        def health_check_immediate():
            return {'status': 'healthy', 'service': 'savlink-backend'}, 200
        
        @app.route('/ping')
        def ping_immediate():
            return 'pong', 200
        
        # Start the server
        app.run(
            host=host,
            port=port,
            debug=debug,
            threaded=True,
            use_reloader=False
        )
        
    except KeyboardInterrupt:
        logger.info("üõë Server stopped by user")
    except Exception as e:
        logger.error(f"‚ùå Server error: {e}")
        sys.exit(1)

# Create app instance for Gunicorn - CRITICAL FIX
def create_app_for_gunicorn():
    """Create app specifically for Gunicorn with immediate routes"""
    app = create_application()
    
    # Add immediate health routes for Render detection
    @app.route('/health')
    def health_immediate():
        return {'status': 'healthy', 'service': 'savlink-backend', 'port': os.environ.get('PORT', '10000')}, 200
    
    @app.route('/ping') 
    def ping_immediate():
        return 'pong', 200
    
    @app.route('/ready')
    def ready_immediate():
        return {'ready': True, 'service': 'savlink-backend'}, 200
    
    return app

# Create app instance for Gunicorn
app = create_app_for_gunicorn()

if __name__ == '__main__':
    main()



2. # server/app/__init__.py
import os
import logging
import time
import threading
from flask import Flask, jsonify, request, make_response, g
from flask_cors import CORS
from sqlalchemy import text
from .config import Config, get_config
from .extensions import db, migrate, redis_client
from .database import db_manager
import json
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Global initialization state
_app_initialized = False
_initialization_lock = threading.Lock()

def create_app(config_class=None):
    """Create and configure the Flask application with enhanced error handling"""
    global _app_initialized
    
    if config_class is None:
        config_class = get_config()
    
    app = Flask(__name__)
    app.config.from_object(config_class)
    
    # Initialize configuration
    config_class.init_app(app)
    
    logger.info(f"üöÄ Starting Savlink backend in {app.config.get('FLASK_ENV', 'production')} mode")
    
    # Step 1: Initialize core extensions (no DB required)
    initialize_core_extensions(app)
    
    # Step 2: Configure request handling
    configure_request_handling(app)
    
    # Step 3: Register endpoints early (for health checks)
    register_root_endpoints(app)
    
    # Step 4: Initialize database in background for production
    if app.config.get('FLASK_ENV') == 'production':
        initialize_database_async(app)
    else:
        # Synchronous for development
        initialize_database_sync(app)
    
    # Step 5: Register blueprints and error handlers
    register_blueprints(app)
    register_error_handlers(app)
    
    logger.info(f"‚úÖ Savlink backend initialized successfully")
    
    return app

def initialize_core_extensions(app):
    """Initialize Flask extensions that don't require database"""
    try:
        # Database ORM (no connection yet)
        db.init_app(app)
        migrate.init_app(app, db)
        
        # Initialize database manager
        db_manager.init_app(app)
        
        # ENHANCED CORS configuration for OAuth compatibility
        cors_origins = app.config.get('CORS_ORIGINS') or []  # FIXED: Handle None case
        
        CORS(app,
             origins=cors_origins,
             supports_credentials=True,
             allow_headers=[
                 'Content-Type', 
                 'Authorization', 
                 'X-Requested-With',
                 'Accept',
                 'Origin',
                 'Access-Control-Request-Method',
                 'Access-Control-Request-Headers'
             ],
             methods=['GET', 'POST', 'PUT', 'DELETE', 'OPTIONS', 'PATCH'],
             expose_headers=['Content-Type', 'Authorization'],
             max_age=86400)
        
        logger.info("‚úÖ Extensions initialized successfully")
        logger.info(f"üåê CORS origins: {len(cors_origins)} configured")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize extensions: {e}", exc_info=True)
        raise

def is_oauth_related_origin(origin):
    """Check if the origin is related to OAuth flows"""
    if not origin:
        return False
    
    oauth_domains = [
        'firebase',
        'googleapis',
        'accounts.google',
        'oauth2.googleapis',
        'www.googleapis',
        'securetoken.google',
        'identitytoolkit.googleapis'
    ]
    
    origin_lower = origin.lower()
    return any(domain in origin_lower for domain in oauth_domains)

def configure_request_handling(app):
    """Configure request timeout and performance monitoring - OAUTH OPTIMIZED"""
    
    @app.before_request
    def before_request():
        """Track request start time and handle CORS preflight"""
        g.request_start_time = time.time()
        
        if app.config.get('DEBUG'):
            logger.debug(f"Request: {request.method} {request.path} from {request.headers.get('Origin', 'unknown')}")
        
        # Enhanced OPTIONS handling for OAuth flows
        if request.method == "OPTIONS":
            response = make_response()
            
            origin = request.headers.get('Origin', '')
            allowed_origins = app.config.get('CORS_ORIGINS', [])
            
            # Check if origin is allowed
            origin_allowed = False
            
            # Direct match
            if origin in allowed_origins:
                origin_allowed = True
            # OAuth-related origins
            elif is_oauth_related_origin(origin):
                origin_allowed = True
            # Wildcard patterns for Vercel deployments
            elif origin and any(
                re.match(pattern.replace('*', '.*'), origin) 
                for pattern in allowed_origins 
                if '*' in pattern
            ):
                origin_allowed = True
            
            if origin_allowed:
                response.headers['Access-Control-Allow-Origin'] = origin
            elif allowed_origins:
                response.headers['Access-Control-Allow-Origin'] = allowed_origins[0]
            else:
                response.headers['Access-Control-Allow-Origin'] = '*'
            
            # Enhanced headers for OAuth compatibility
            response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, DELETE, OPTIONS, PATCH'
            response.headers['Access-Control-Allow-Headers'] = (
                'Content-Type, Authorization, X-Requested-With, Accept, Origin, '
                'Access-Control-Request-Method, Access-Control-Request-Headers'
            )
            response.headers['Access-Control-Allow-Credentials'] = 'true'
            response.headers['Access-Control-Max-Age'] = '86400'
            
            # CRITICAL: OAuth-friendly COOP headers
            response.headers['Cross-Origin-Opener-Policy'] = 'unsafe-none'
            response.headers['Cross-Origin-Embedder-Policy'] = 'unsafe-none'
            
            # Additional headers for OAuth flows
            response.headers['Vary'] = 'Origin, Access-Control-Request-Method, Access-Control-Request-Headers'
            
            return response, 200
    
    @app.after_request
    def after_request(response):
        """Add security headers and monitor performance - OAUTH OPTIMIZED"""
        if hasattr(g, 'request_start_time'):
            duration = time.time() - g.request_start_time
            
            threshold = app.config.get('SLOW_REQUEST_THRESHOLD', 15)
            if duration > threshold:
                logger.warning(f"üêå Slow request: {request.method} {request.path} took {duration:.2f}s")
            
            if app.config.get('DEBUG'):
                response.headers['X-Response-Time'] = f"{duration:.3f}s"
        
        origin = request.headers.get('Origin', '')
        
        # ENHANCED: OAuth-friendly COOP headers based on origin
        if is_oauth_related_origin(origin):
            # Most permissive for OAuth flows
            response.headers['Cross-Origin-Opener-Policy'] = 'unsafe-none'
        elif origin and any(domain in origin for domain in ['vercel.app', 'localhost', '127.0.0.1']):
            # Permissive for our own domains
            response.headers['Cross-Origin-Opener-Policy'] = 'same-origin-allow-popups'
        else:
            # Default for other origins
            response.headers['Cross-Origin-Opener-Policy'] = 'same-origin-allow-popups'
        
        # Always use unsafe-none for COEP to avoid blocking OAuth
        response.headers['Cross-Origin-Embedder-Policy'] = 'unsafe-none'
        
        # Security headers - OAuth optimized
        response.headers['X-Content-Type-Options'] = 'nosniff'
        response.headers['X-Frame-Options'] = 'SAMEORIGIN'  # Less restrictive for OAuth
        response.headers['X-XSS-Protection'] = '1; mode=block'
        response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'
        
        # Vary header for proper caching with CORS
        existing_vary = response.headers.get('Vary', '')
        vary_values = ['Origin']
        if existing_vary:
            vary_values = list(set(existing_vary.split(', ') + vary_values))
        response.headers['Vary'] = ', '.join(vary_values)
        
        # Cache control for API responses
        if request.path.startswith('/api/') or request.path.startswith('/auth/'):
            response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'
            response.headers['Pragma'] = 'no-cache'
            response.headers['Expires'] = '0'
        elif request.path.startswith('/r/'):  # Short links should be cacheable
            response.headers['Cache-Control'] = 'public, max-age=300'  # 5 minutes
        
        return response
    
    @app.teardown_request
    def teardown_request(exception):
        """Log any request exceptions"""
        if exception:
            logger.error(f"‚ùå Request exception: {exception}", exc_info=True)

def initialize_database_async(app):
    """Initialize database asynchronously for production"""
    
    def init_task():
        with app.app_context():
            try:
                logger.info("üîÑ Starting async database initialization...")
                
                # Initialize database with retry
                result = db_manager.initialize_with_retry()
                
                if result['success']:
                    # Run migrations
                    try:
                        run_database_migrations(app)
                    except Exception as e:
                        logger.error(f"‚ùå Migration failed but continuing: {e}")
                    
                    # Initialize Firebase
                    try:
                        initialize_firebase(app)
                    except Exception as e:
                        logger.error(f"‚ùå Firebase init failed but continuing: {e}")
                else:
                    logger.warning(f"‚ö†Ô∏è Database initialization failed: {result.get('error')}")
                    
            except Exception as e:
                logger.error(f"‚ùå Async database initialization failed: {e}", exc_info=True)
    
    # Start background initialization
    thread = threading.Thread(target=init_task, daemon=True)
    thread.start()
    
    logger.info("üîÑ Database initialization started in background")

def initialize_database_sync(app):
    """Initialize database synchronously for development"""
    with app.app_context():
        try:
            logger.info("üîÑ Initializing database synchronously...")
            
            result = db_manager.initialize_with_retry()
            
            if result['success']:
                logger.info("‚úÖ Database initialized successfully")
                
                # Run migrations
                run_database_migrations(app)
                
                # Initialize Firebase
                initialize_firebase(app)
            else:
                logger.warning(f"‚ö†Ô∏è Database initialization failed: {result.get('error')}")
                # Continue anyway for development
                
        except Exception as e:
            logger.error(f"‚ùå Database initialization error: {e}", exc_info=True)
            # Don't raise in development to allow debugging

def run_database_migrations(app):
    """Run database migrations with enhanced error handling"""
    try:
        logger.info("üîÑ Checking for database migrations...")
        
        from .migrations.manager import migration_manager
        from .migrations import register_all_migrations
        
        # Register all migrations
        register_all_migrations()
        
        # Run migrations with retry
        result = migration_manager.run_migrations_with_enhanced_retry(dry_run=False)
        
        if result['status'] == 'success':
            if result['migrations_run'] > 0:
                logger.info(f"‚úÖ Applied {result['migrations_run']} migrations")
                for detail in result['details']:
                    if detail['status'] == 'success':
                        logger.info(f"  ‚úÖ {detail['version']} ({detail.get('execution_time_ms', 0)}ms)")
                    elif detail['status'] == 'skipped':
                        logger.info(f"  ‚è≠Ô∏è {detail['version']} (skipped - {detail.get('reason', 'unknown')})")
            else:
                logger.info("‚úÖ Database schema is up to date")
        else:
            logger.error(f"‚ùå Migration failed: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        logger.error(f"‚ùå Migration process error: {e}", exc_info=True)

def initialize_firebase(app):
    """Initialize Firebase Admin SDK with error handling"""
    try:
        from .auth.firebase import initialize_firebase as init_firebase
        
        if not app.config.get('FIREBASE_CONFIG_JSON'):
            logger.warning("‚ö†Ô∏è FIREBASE_CONFIG_JSON not configured - Firebase auth disabled")
            return
        
        firebase_app = init_firebase()
        
        if firebase_app:
            logger.info("‚úÖ Firebase Admin SDK initialized")
        else:
            logger.warning("‚ö†Ô∏è Firebase Admin SDK initialization returned None")
            
    except ValueError as ve:
        logger.error(f"‚ùå Firebase configuration error: {ve}")
    except Exception as e:
        logger.error(f"‚ùå Firebase initialization error: {e}")

def register_blueprints(app):
    """Register all application blueprints"""
    try:
        # Auth blueprint
        from .auth import auth_bp
        app.register_blueprint(auth_bp, url_prefix='/auth')
        logger.info("‚úÖ Auth blueprint registered")
        
        # Links blueprint
        from .links import links_bp
        app.register_blueprint(links_bp, url_prefix='/api/links')
        logger.info("‚úÖ Links blueprint registered")
        
        # Dashboard blueprint
        from .dashboard import dashboard_bp
        app.register_blueprint(dashboard_bp, url_prefix='/api/dashboard')
        logger.info("‚úÖ Dashboard blueprint registered")
        
        # Redirect blueprint
        from .redirect import redirect_bp
        app.register_blueprint(redirect_bp, url_prefix='/r')
        logger.info("‚úÖ Redirect blueprint registered")
        
        # Folders blueprint
        from .folders import folders_bp
        app.register_blueprint(folders_bp, url_prefix='/api/folders')
        logger.info("‚úÖ Folders blueprint registered")
        
        # Tags blueprint
        from .tags import tags_bp
        app.register_blueprint(tags_bp, url_prefix='/api/tags')
        logger.info("‚úÖ Tags blueprint registered")
        
        # Search blueprint
        from .search import search_bp
        app.register_blueprint(search_bp, url_prefix='/api/search')
        logger.info("‚úÖ Search blueprint registered")
        
        # Shortlinks blueprint
        from .shortlinks import shortlinks_bp
        app.register_blueprint(shortlinks_bp, url_prefix='/api/shortlinks')
        logger.info("‚úÖ Shortlinks blueprint registered")
        
        # NEW: Metadata blueprint
        from .metadata.routes import metadata_bp
        app.register_blueprint(metadata_bp, url_prefix='/api/metadata')
        logger.info("‚úÖ Metadata blueprint registered")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to register blueprints: {e}", exc_info=True)
        raise

def register_error_handlers(app):
    """Register error handlers for the application"""
    
    @app.errorhandler(400)
    def bad_request(error):
        return jsonify({'success': False, 'error': 'Bad request', 'message': str(error)}), 400
    
    @app.errorhandler(401)
    def unauthorized(error):
        response = jsonify({'success': False, 'error': 'Unauthorized', 'message': 'Authentication required'})
        # Add CORS headers for OAuth error responses
        origin = request.headers.get('Origin', '')
        if origin and (origin in app.config.get('CORS_ORIGINS', []) or is_oauth_related_origin(origin)):
            response.headers['Access-Control-Allow-Origin'] = origin
            response.headers['Access-Control-Allow-Credentials'] = 'true'
        return response, 401
    
    @app.errorhandler(403)
    def forbidden(error):
        return jsonify({'success': False, 'error': 'Forbidden', 'message': 'Access denied'}), 403
    
    @app.errorhandler(404)
    def not_found(error):
        return jsonify({'success': False, 'error': 'Not found', 'message': 'Resource not found'}), 404
    
    @app.errorhandler(429)
    def rate_limit_exceeded(error):
        return jsonify({'success': False, 'error': 'Too many requests', 'message': 'Rate limit exceeded'}), 429
    
    @app.errorhandler(500)
    def internal_error(error):
        logger.error(f"‚ùå Internal server error: {error}", exc_info=True)
        return jsonify({'success': False, 'error': 'Internal server error', 'message': 'Something went wrong'}), 500
    
    @app.errorhandler(503)
    def service_unavailable(error):
        return jsonify({'success': False, 'error': 'Service unavailable', 'message': 'Service temporarily unavailable'}), 503
    
    @app.errorhandler(504)
    def gateway_timeout(error):
        return jsonify({'success': False, 'error': 'Gateway timeout', 'message': 'Request timed out'}), 504
    
    @app.errorhandler(Exception)
    def unhandled_exception(error):
        logger.error(f"‚ùå Unhandled exception: {error}", exc_info=True)
        
        if app.config.get('DEBUG'):
            return jsonify({
                'success': False,
                'error': error.__class__.__name__,
                'message': str(error)
            }), 500
        else:
            return jsonify({
                'success': False,
                'error': 'Server error',
                'message': 'An unexpected error occurred'
            }), 500

def register_root_endpoints(app):
    """Register root-level endpoints with immediate health checks"""
    
    @app.route('/')
    def index():
        """Root endpoint - API information"""
        return jsonify({
            'service': 'savlink-backend',
            'version': '3.0.0',
            'status': 'online',
            'timestamp': time.time(),
            'environment': app.config.get('FLASK_ENV', 'production'),
            'message': 'Savlink API is running successfully',
            'port': os.environ.get('PORT', '10000')
        })
    
    @app.route('/ping')
    def ping():
        """Ultra-lightweight ping endpoint"""
        return 'pong', 200, {'Content-Type': 'text/plain'}
    
    @app.route('/health')
    def health_check():
        """Immediate health check for Render"""
        # Return immediately without database checks for port detection
        return jsonify({
            'status': 'healthy',
            'service': 'savlink-backend',
            'version': '3.0.0',
            'timestamp': time.time(),
            'environment': app.config.get('FLASK_ENV', 'production'),
            'port': os.environ.get('PORT', '10000'),
            'message': 'Service is running'
        })
    
    @app.route('/health/full')
    def health_check_full():
        """Comprehensive health check endpoint"""
        start_time = time.time()
        health_status = {
            'status': 'healthy',
            'service': 'savlink-backend',
            'version': '3.0.0',
            'timestamp': time.time(),
            'environment': app.config.get('FLASK_ENV', 'production'),
            'checks': {}
        }
        
        # Check database
        db_health = db_manager.check_health()
        health_status['checks']['database'] = db_health
        
        if not db_health.get('healthy', False):
            health_status['status'] = 'degraded'
        
        # Check Redis if available
        try:
            if redis_client.available:
                redis_start = time.time()
                redis_healthy = redis_client.ping()
                redis_time = (time.time() - redis_start) * 1000
                
                health_status['checks']['redis'] = {
                    'status': 'healthy' if redis_healthy else 'unhealthy',
                    'response_time_ms': round(redis_time, 2)
                }
            else:
                health_status['checks']['redis'] = {'status': 'not_configured'}
        except Exception as e:
            health_status['checks']['redis'] = {'status': 'error', 'error': str(e)}
        
        # Check Firebase
        try:
            from .auth.firebase import _firebase_app
            health_status['checks']['firebase'] = {
                'status': 'healthy' if _firebase_app else 'not_initialized'
            }
        except Exception:
            health_status['checks']['firebase'] = {'status': 'not_configured'}
        
        # Calculate total response time
        total_time = (time.time() - start_time) * 1000
        health_status['response_time_ms'] = round(total_time, 2)
        
        return jsonify(health_status)
    
    @app.route('/ready')
    def readiness_check():
        """Kubernetes readiness probe"""
        try:
            # Quick database check
            db_health = db_manager.check_health()
            
            if db_health.get('healthy', False):
                return jsonify({'ready': True, 'service': 'savlink-backend'}), 200
            else:
                return jsonify({'ready': False, 'service': 'savlink-backend', 'reason': 'database_unhealthy'}), 503
                
        except Exception as e:
            logger.error(f"‚ùå Readiness check failed: {e}")
            return jsonify({'ready': False, 'service': 'savlink-backend', 'error': str(e)}), 503
    
    @app.route('/db-status')
    def database_status():
        """Detailed database status endpoint"""
        try:
            health = db_manager.check_health()
            connection_info = db_manager.get_connection_info()
            
            return jsonify({
                'database': {
                    'health': health,
                    'connection': connection_info,
                    'timestamp': time.time()
                }
            }), 200 if health.get('healthy', False) else 503
            
        except Exception as e:
            return jsonify({
                'database': {
                    'error': str(e),
                    'timestamp': time.time()
                }
            }), 500
    
    @app.route('/cors-test')
    def cors_test():
        """Test CORS configuration"""
        origin = request.headers.get('Origin', 'none')
        return jsonify({
            'message': 'CORS test successful',
            'origin': origin,
            'oauth_related': is_oauth_related_origin(origin),
            'allowed_origins': len(app.config.get('CORS_ORIGINS', [])),
            'timestamp': time.time()
        })




3. # server/app/config.py
import os
import json
from datetime import timedelta
import logging

logger = logging.getLogger(__name__)

def build_cors_origins_list(include_dev_origins=False):
    """Build CORS origins list from environment variables"""
    # Get base values
    base_url = os.environ.get('BASE_URL', 'https://savlink.vercel.app')
    firebase_config_json = os.environ.get('FIREBASE_CONFIG_JSON')
    
    cors_origins = [
        # Development
        'http://localhost:5173',
        'http://localhost:3000',
        'http://localhost:5000',
        'http://127.0.0.1:5173',
        'http://127.0.0.1:3000',
        'http://127.0.0.1:5000',
        
        # Production domains
        'https://savlink.vercel.app',
        'https://*.vercel.app',  # All Vercel deployments
        base_url,
        
        # Firebase Auth domains
        'https://savlink-f83ef.firebaseapp.com',
        'https://*.firebaseapp.com',
        'https://firebase.google.com',
        
        # Google OAuth domains
        'https://accounts.google.com',
        'https://oauth2.googleapis.com',
        'https://www.googleapis.com',
        'https://securetoken.google.com',
        'https://identitytoolkit.googleapis.com',
        
        # Additional OAuth providers if needed
        'https://api.github.com',
        'https://github.com'
    ]
    
    # Add development origins if requested
    if include_dev_origins:
        dev_origins = [
            'http://localhost:*',
            'http://127.0.0.1:*',
            'https://*.ngrok.io',
            'https://*.localtunnel.me'
        ]
        cors_origins.extend(dev_origins)
    
    # Parse additional origins from environment
    additional_origins = os.environ.get('ADDITIONAL_CORS_ORIGINS', '')
    if additional_origins:
        try:
            # Split by comma and clean up whitespace
            extra_origins = [origin.strip() for origin in additional_origins.split(',') if origin.strip()]
            cors_origins.extend(extra_origins)
        except Exception as e:
            logger.warning(f"Failed to parse ADDITIONAL_CORS_ORIGINS: {e}")
    
    # Parse Firebase project ID to add specific domains
    if firebase_config_json:
        try:
            config_dict = json.loads(firebase_config_json)
            project_id = config_dict.get('project_id')
            if project_id:
                # Add project-specific Firebase domains
                firebase_domains = [
                    f'https://{project_id}.firebaseapp.com',
                    f'https://{project_id}.web.app',
                    f'https://{project_id}-default-rtdb.firebaseio.com'
                ]
                cors_origins.extend(firebase_domains)
        except (json.JSONDecodeError, KeyError, TypeError) as e:
            logger.warning(f"Failed to parse Firebase config for CORS origins: {e}")
    
    # Remove duplicates while preserving order
    seen = set()
    unique_origins = []
    for origin in cors_origins:
        if origin and origin not in seen:
            seen.add(origin)
            unique_origins.append(origin)
    
    return unique_origins

class Config:
    """Production-optimized configuration for Savlink backend with OAuth support"""
    
    # Security
    SECRET_KEY = os.environ.get('SECRET_KEY') or os.urandom(32).hex()
    
    # Database Configuration - OPTIMIZED FOR SUPABASE
    # CRITICAL FIX: Strip whitespace from DATABASE_URL
    DATABASE_URL = os.environ.get('DATABASE_URL', '').strip()
    SQLALCHEMY_DATABASE_URI = DATABASE_URL
    
    # Fix for SQLAlchemy with newer PostgreSQL URLs
    if SQLALCHEMY_DATABASE_URI and SQLALCHEMY_DATABASE_URI.startswith('postgres://'):
        SQLALCHEMY_DATABASE_URI = SQLALCHEMY_DATABASE_URI.replace('postgres://', 'postgresql://', 1)
    
    # Additional validation to ensure clean URL
    if SQLALCHEMY_DATABASE_URI:
        # Remove any trailing whitespace, newlines, or carriage returns
        SQLALCHEMY_DATABASE_URI = SQLALCHEMY_DATABASE_URI.strip().rstrip('\n').rstrip('\r')
        
        # Log the database host for debugging (without credentials)
        try:
            from urllib.parse import urlparse
            parsed = urlparse(SQLALCHEMY_DATABASE_URI)
            logger.info(f"Database host: {parsed.hostname}, Database name: {parsed.path.lstrip('/')}")
        except Exception:
            pass
    
    SQLALCHEMY_TRACK_MODIFICATIONS = False
    
    # ENHANCED connection pooling specifically for Supabase
    SQLALCHEMY_ENGINE_OPTIONS = {
        'pool_size': 1,              # Minimal pool for cold starts  
        'max_overflow': 2,           # Small overflow
        'pool_recycle': 300,         # 5 minutes - shorter for hosted
        'pool_pre_ping': False,      # DISABLED - was causing connection issues
        'pool_timeout': 60,          # Longer timeout
        'pool_reset_on_return': None, # Don't reset connections
        'echo': False,               # Disable SQL logging in production
        'connect_args': {
            'connect_timeout': 30,
            'application_name': 'savlink_backend',
            'sslmode': 'require',    # Force SSL for Supabase
            'options': '-c statement_timeout=30000 -c lock_timeout=10000'
        }
    }
    
    # Connection retry settings
    DATABASE_RETRY_ATTEMPTS = 8
    DATABASE_RETRY_BASE_DELAY = 3
    DATABASE_MAX_DELAY = 60
    
    # Firebase Configuration
    FIREBASE_CONFIG_JSON = os.environ.get('FIREBASE_CONFIG_JSON')
    
    # Application URLs
    BASE_URL = os.environ.get('BASE_URL', 'https://savlink.vercel.app')
    API_URL = os.environ.get('API_URL')
    
    # Short link redirect prefix
    SHORT_LINK_PREFIX = '/r'
    
    # Redis Configuration (Optional)
    REDIS_URL = os.environ.get('REDIS_URL')
    
    # Redis settings for session/cache
    REDIS_OPTIONS = {
        'decode_responses': True,
        'socket_connect_timeout': 10,
        'socket_timeout': 10,
        'retry_on_timeout': True,
        'health_check_interval': 60,
        'max_connections': 20
    } if REDIS_URL else {}
    
    # Email Configuration (Brevo/SendinBlue)
    BREVO_API_KEY = os.environ.get('BREVO_API_KEY')
    EMAIL_FROM_NAME = os.environ.get('EMAIL_FROM_NAME', 'Savlink')
    EMAIL_FROM_ADDRESS = os.environ.get('EMAIL_FROM_ADDRESS', 'noreply@savlink.com')
    USE_BREVO_API = os.environ.get('USE_BREVO_API', 'true').lower() == 'true'
    
    # Emergency Access Configuration
    EMERGENCY_TOKEN_TTL = timedelta(minutes=15)
    EMERGENCY_SESSION_TTL = timedelta(hours=1)
    EMERGENCY_RATE_LIMIT = 3
    EMERGENCY_VERIFY_LIMIT = 10
    
    # Session Configuration
    SESSION_TOKEN_LENGTH = 32
    SESSION_COOKIE_SECURE = os.environ.get('FLASK_ENV') == 'production'
    SESSION_COOKIE_HTTPONLY = True
    SESSION_COOKIE_SAMESITE = 'Lax'
    
    # Request Configuration - OPTIMIZED FOR COLD STARTS
    MAX_CONTENT_LENGTH = 16 * 1024 * 1024
    REQUEST_TIMEOUT = 60  # Increased for cold starts
    
    # Rate Limiting (if Redis available)
    RATELIMIT_ENABLED = bool(REDIS_URL)
    RATELIMIT_STORAGE_URL = REDIS_URL if REDIS_URL else None
    RATELIMIT_DEFAULT = "100 per hour"
    RATELIMIT_HEADERS_ENABLED = True
    
    # Dashboard defaults
    DASHBOARD_DEFAULT_LIMIT = 20
    DASHBOARD_MAX_LIMIT = 100
    
    # Logging Configuration
    LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO')
    LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Performance Monitoring
    SLOW_REQUEST_THRESHOLD = 15  # Increased for cold starts
    
    # Environment
    FLASK_ENV = os.environ.get('FLASK_ENV', 'production')
    DEBUG = FLASK_ENV == 'development'
    TESTING = FLASK_ENV == 'testing'
    
    # Feature Flags
    ENABLE_METRICS = os.environ.get('ENABLE_METRICS', 'false').lower() == 'true'
    ENABLE_PROFILING = os.environ.get('ENABLE_PROFILING', 'false').lower() == 'true'
    ENABLE_OAUTH_DEBUG = os.environ.get('ENABLE_OAUTH_DEBUG', 'false').lower() == 'true'
    
    # Cold start optimization
    ASYNC_DB_INIT = FLASK_ENV == 'production'
    
    # OAuth-specific configurations
    OAUTH_POPUP_TIMEOUT = 60  # seconds
    OAUTH_REDIRECT_TIMEOUT = 120  # seconds
    OAUTH_STATE_TTL = 300  # 5 minutes
    
    # Build CORS origins (production defaults)
    CORS_ORIGINS = build_cors_origins_list(include_dev_origins=False)
    
    @classmethod
    def init_app(cls, app):
        """Initialize application with configuration"""
        # Ensure CORS origins are set on the app config
        app.config['CORS_ORIGINS'] = cls.CORS_ORIGINS
        
        # Set up logging
        logging.basicConfig(
            level=getattr(logging, cls.LOG_LEVEL),
            format=cls.LOG_FORMAT
        )
        
        if cls.DEBUG or cls.ENABLE_OAUTH_DEBUG:
            app.logger.debug('üîß Configuration loaded:')
            app.logger.debug(f'  - Database: {bool(cls.SQLALCHEMY_DATABASE_URI)}')
            app.logger.debug(f'  - Redis: {bool(cls.REDIS_URL)}')
            app.logger.debug(f'  - Firebase: {bool(cls.FIREBASE_CONFIG_JSON)}')
            app.logger.debug(f'  - Email: {bool(cls.BREVO_API_KEY)}')
            app.logger.debug(f'  - Base URL: {cls.BASE_URL}')
            app.logger.debug(f'  - Environment: {cls.FLASK_ENV}')
            app.logger.debug(f'  - Async DB Init: {cls.ASYNC_DB_INIT}')
            app.logger.debug(f'  - CORS Origins: {len(cls.CORS_ORIGINS)} configured')
            
            if cls.ENABLE_OAUTH_DEBUG:
                app.logger.debug('üîê OAuth Debug - CORS Origins:')
                for i, origin in enumerate(cls.CORS_ORIGINS[:10]):  # Show first 10
                    app.logger.debug(f'    - {origin}')
                if len(cls.CORS_ORIGINS) > 10:
                    app.logger.debug(f'    ... and {len(cls.CORS_ORIGINS) - 10} more')

class DevelopmentConfig(Config):
    """Development configuration with enhanced OAuth debugging"""
    DEBUG = True
    FLASK_ENV = 'development'
    LOG_LEVEL = 'DEBUG'
    ENABLE_OAUTH_DEBUG = True
    
    SQLALCHEMY_ENGINE_OPTIONS = {
        'pool_size': 5,
        'pool_recycle': 3600,
        'pool_pre_ping': True,
        'echo': False
    }
    
    ASYNC_DB_INIT = False  # Sync for development
    
    # Override CORS origins for development (include dev origins)
    CORS_ORIGINS = build_cors_origins_list(include_dev_origins=True)

class ProductionConfig(Config):
    """Production configuration with OAuth optimizations"""
    DEBUG = False
    FLASK_ENV = 'production'
    LOG_LEVEL = 'INFO'
    
    SESSION_COOKIE_SECURE = True
    ASYNC_DB_INIT = True  # Async for production
    
    # Production OAuth settings
    OAUTH_POPUP_TIMEOUT = 30  # Shorter timeout in production
    OAUTH_REDIRECT_TIMEOUT = 60
    
    @classmethod
    def init_app(cls, app):
        # Call parent init_app first
        Config.init_app(app)
        
        # Production logging setup
        import logging
        from logging import StreamHandler
        
        # Console handler for production logs
        console_handler = StreamHandler()
        console_handler.setLevel(logging.INFO)
        formatter = logging.Formatter(cls.LOG_FORMAT)
        console_handler.setFormatter(formatter)
        
        # Remove default handlers and add our own
        app.logger.handlers.clear()
        app.logger.addHandler(console_handler)
        app.logger.setLevel(logging.INFO)
        
        # Log OAuth configuration in production (without sensitive data)
        app.logger.info(f"üîê OAuth configured with {len(cls.CORS_ORIGINS)} CORS origins")
        app.logger.info(f"üöÄ Production mode initialized")
        
        # Validate critical config
        if not cls.SECRET_KEY or cls.SECRET_KEY == 'dev-secret-key':
            app.logger.error("‚ùå SECRET_KEY not set or using default value!")
            raise ValueError("SECRET_KEY must be set in production")
        
        if not cls.SQLALCHEMY_DATABASE_URI:
            app.logger.error("‚ùå DATABASE_URL not configured!")
            raise ValueError("DATABASE_URL must be set in production")

class TestingConfig(Config):
    """Testing configuration"""
    TESTING = True
    FLASK_ENV = 'testing'
    SQLALCHEMY_DATABASE_URI = 'sqlite:///:memory:'
    WTF_CSRF_ENABLED = False
    
    RATELIMIT_ENABLED = False
    ASYNC_DB_INIT = False
    
    # Minimal CORS for testing
    CORS_ORIGINS = ['http://localhost:3000', 'http://localhost:5173']

# Configuration registry
config = {
    'development': DevelopmentConfig,
    'production': ProductionConfig,
    'testing': TestingConfig,
    'default': ProductionConfig
}

def get_config():
    """Get configuration based on environment"""
    env = os.environ.get('FLASK_ENV', 'production')
    config_class = config.get(env, ProductionConfig)
    
    # Log which config is being used
    print(f"üîß Using {config_class.__name__} configuration (env: {env})")
    
    return config_class

def validate_config():
    """Validate configuration for production readiness"""
    env = os.environ.get('FLASK_ENV', 'production')
    
    if env == 'production':
        required_vars = [
            'SECRET_KEY',
            'DATABASE_URL',
            'FIREBASE_CONFIG_JSON',
        ]
        
        missing_vars = []
        for var in required_vars:
            if not os.environ.get(var):
                missing_vars.append(var)
        
        if missing_vars:
            raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}")
    
    return True

# Validate configuration on import in production
if os.environ.get('FLASK_ENV') == 'production':
    try:
        validate_config()
        print("‚úÖ Configuration validation passed")
    except ValueError as e:
        print(f"‚ùå Configuration validation failed: {e}")




4. # server/app/models/__init__.py
from .user import User
from .emergency_token import EmergencyToken
from .link import Link
from .folder import Folder
from .tag import Tag
from .link_tag import LinkTag

__all__ = ['User', 'EmergencyToken', 'Link', 'Folder', 'Tag', 'LinkTag']




5. from datetime import datetime
from app.extensions import db

class Folder(db.Model):
    __tablename__ = 'folders'
    
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Text, db.ForeignKey('users.id'), nullable=False, index=True)
    parent_id = db.Column(db.Integer, db.ForeignKey('folders.id'), nullable=True, index=True)  # NEW
    name = db.Column(db.String(255), nullable=False)
    color = db.Column(db.String(7), nullable=True)
    icon = db.Column(db.String(50), nullable=True)
    pinned = db.Column(db.Boolean, default=False, nullable=False, index=True)  # NEW
    position = db.Column(db.Integer, nullable=False, default=0)
    created_at = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)
    updated_at = db.Column(db.DateTime, nullable=False, default=datetime.utcnow, onupdate=datetime.utcnow)
    soft_deleted = db.Column(db.Boolean, default=False, nullable=False, index=True)
    
    user = db.relationship('User', backref=db.backref('folders', lazy='dynamic'))
    parent = db.relationship('Folder', remote_side=[id], backref='children')  # NEW
    
    __table_args__ = (
        db.Index('ix_folders_user_active', 'user_id', 'soft_deleted'),
        db.Index('ix_folders_user_position', 'user_id', 'position', 'soft_deleted'),
        db.Index('ix_folders_parent', 'parent_id', 'user_id', 'soft_deleted'),  # NEW
        db.Index('ix_folders_user_pinned', 'user_id', 'pinned', 'soft_deleted'),  # NEW
        db.UniqueConstraint('user_id', 'name', 'soft_deleted', name='uq_user_folder_name'),
    )




6. # server/app/models/link_tag.py

from datetime import datetime
from app.extensions import db
from sqlalchemy.orm import relationship  # ADD THIS

class LinkTag(db.Model):
    __tablename__ = 'link_tags'
    
    id = db.Column(db.Integer, primary_key=True)
    link_id = db.Column(db.Integer, db.ForeignKey('links.id'), nullable=False)
    tag_id = db.Column(db.Integer, db.ForeignKey('tags.id'), nullable=False)
    user_id = db.Column(db.Text, db.ForeignKey('users.id'), nullable=False, index=True)
    created_at = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)
    
    # ‚úÖ ADD THESE RELATIONSHIPS
    link = db.relationship('Link', backref=db.backref('link_tags', lazy='dynamic', cascade='all, delete-orphan'))
    tag = db.relationship('Tag', backref=db.backref('link_tags', lazy='dynamic', cascade='all, delete-orphan'))
    user = db.relationship('User')
    
    __table_args__ = (
        db.Index('ix_link_tags_user', 'user_id'),
        db.Index('ix_link_tags_link', 'link_id'),
        db.Index('ix_link_tags_tag', 'tag_id'),
        db.UniqueConstraint('link_id', 'tag_id', name='uq_link_tag'),
    )




7. # server/app/models/link.py
from datetime import datetime
from app.extensions import db
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import relationship
import logging

logger = logging.getLogger(__name__)

class Link(db.Model):
    __tablename__ = 'links'

    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Text, db.ForeignKey('users.id'), nullable=False, index=True)
    folder_id = db.Column(db.Integer, db.ForeignKey('folders.id'), nullable=True, index=True)
    original_url = db.Column(db.Text, nullable=False)
    link_type = db.Column(db.String(20), nullable=False, default='saved', index=True)
    slug = db.Column(db.String(255), unique=True, nullable=True, index=True)
    title = db.Column(db.String(500))
    notes = db.Column(db.Text)
    is_active = db.Column(db.Boolean, default=True, nullable=False)
    pinned = db.Column(db.Boolean, default=False, nullable=False, index=True)
    starred = db.Column(db.Boolean, default=False, nullable=False, index=True)
    frequently_used = db.Column(db.Boolean, default=False, nullable=False, index=True)
    pinned_at = db.Column(db.DateTime, nullable=True)
    archived_at = db.Column(db.DateTime, nullable=True, index=True)
    expires_at = db.Column(db.DateTime, nullable=True, index=True)
    created_at = db.Column(db.DateTime, nullable=False, default=datetime.utcnow, index=True)
    updated_at = db.Column(db.DateTime, nullable=False, default=datetime.utcnow, onupdate=datetime.utcnow)
    soft_deleted = db.Column(db.Boolean, default=False, nullable=False, index=True)
    click_count = db.Column(db.Integer, default=0, nullable=False)
    metadata_ = db.Column('metadata', JSONB, default=dict)
    
    # ‚úÖ ADD THIS COLUMN
    password_hash = db.Column(db.String(255), nullable=True)

    # Relationships
    user = db.relationship('User', backref=db.backref('links', lazy='dynamic'))
    folder = db.relationship('Folder', backref=db.backref('links', lazy='dynamic'))
    
    tags = relationship(
        'Tag',
        secondary='link_tags',
        lazy='selectin',
        viewonly=True
    )
    
    __table_args__ = (
        db.Index('ix_links_user_active', 'user_id', 'soft_deleted', 'archived_at'),
        db.Index('ix_links_user_pinned', 'user_id', 'pinned', 'soft_deleted'),
        db.Index('ix_links_user_starred', 'user_id', 'starred', 'soft_deleted'),
        db.Index('ix_links_slug_active', 'slug', 'is_active', 'soft_deleted'),
        db.Index('ix_links_user_created', 'user_id', 'created_at', 'soft_deleted'),
        db.Index('ix_links_user_folder', 'user_id', 'folder_id', 'soft_deleted'),
        db.Index('ix_links_expires', 'expires_at', 'link_type', 'is_active'),
    )

    def __repr__(self):
        return f'<Link {self.id} [{self.link_type}] {self.original_url[:50]}>'




8. # server/app/models/tag.py
from datetime import datetime
from app.extensions import db

class Tag(db.Model):
    __tablename__ = 'tags'
    
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Text, db.ForeignKey('users.id'), nullable=False, index=True)
    name = db.Column(db.String(100), nullable=False)
    color = db.Column(db.String(7), nullable=True)
    created_at = db.Column(db.DateTime, nullable=False, default=datetime.utcnow)
    
    user = db.relationship('User', backref=db.backref('tags', lazy='dynamic'))
    
    __table_args__ = (
        db.Index('ix_tags_user', 'user_id'),
        db.UniqueConstraint('user_id', 'name', name='uq_user_tag_name'),
    )




9. # server/app/dashboard/__init__.py
from flask import Blueprint

dashboard_bp = Blueprint('dashboard', __name__)

from . import routes




10. # server/app/dashboard/service.py

from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from app.models import Link, Folder, Tag
from app.utils.time import relative_time
from app.utils.url import extract_display_url, build_favicon_url, extract_domain
from app.utils.base_url import get_base_url, get_short_link_url
from app.links.tagging import get_link_tags
from app.extensions import redis_client
import json
import logging

logger = logging.getLogger(__name__)

class EnhancedLinkSerializer:
    """Enhanced link serialization with rich metadata and smart features"""
    
    def __init__(self, user_id: str):
        self.user_id = user_id
        self.base_url = get_base_url()
        self._domain_cache = {}
        self._tag_cache = {}
    
    def serialize_link(self, link: Link, include_analytics: bool = False, include_preview: bool = True) -> Dict[str, Any]:
        """Enhanced link serialization with comprehensive metadata"""
        
        # Basic link data
        data = {
            'id': link.id,
            'title': link.title or self._generate_smart_title(link),
            'original_url': link.original_url,
            'display_url': extract_display_url(link.original_url),
            'link_type': link.link_type,
            'is_active': link.is_active,
            'created_at': link.created_at.isoformat() if link.created_at else None,
            'updated_at': link.updated_at.isoformat() if link.updated_at else None,
            'relative_time': relative_time(link.created_at)
        }
        
        # Status flags
        data.update({
            'pinned': link.pinned,
            'pinned_at': link.pinned_at.isoformat() if link.pinned_at else None,
            'starred': getattr(link, 'starred', False),
            'frequently_used': getattr(link, 'frequently_used', False),
            'archived': link.archived_at is not None,
            'archived_at': link.archived_at.isoformat() if link.archived_at else None
        })
        
        # Folder information
        if link.folder_id:
            data['folder_id'] = link.folder_id
            if hasattr(link, 'folder') and link.folder:
                data['folder'] = {
                    'id': link.folder.id,
                    'name': link.folder.name,
                    'color': link.folder.color,
                    'icon': link.folder.icon
                }
        else:
            data['folder_id'] = None
            data['folder'] = None
        
        # Short link specific data
        if link.link_type == 'shortened':
            data.update(self._serialize_short_link_data(link))
        
        # Notes handling
        if link.notes:
            notes_preview = link.notes[:120].strip()
            if len(link.notes) > 120:
                notes_preview += '...'
            data.update({
                'notes': link.notes,
                'notes_preview': notes_preview,
                'has_notes': True
            })
        else:
            data.update({
                'notes': None,
                'notes_preview': None,
                'has_notes': False
            })
        
        # Preview data with enhanced metadata
        if include_preview:
            data['preview'] = self._build_enhanced_preview_data(link)
        
        # Tags with caching
        data['tags'] = self._get_cached_link_tags(link)
        
        # Analytics for short links
        if include_analytics and link.link_type == 'shortened':
            data['analytics'] = self._get_link_analytics_summary(link)
        
        # Smart metadata and scoring
        data['metadata'] = self._build_smart_metadata(link)
        
        # Performance indicators
        data['performance'] = self._calculate_performance_metrics(link)
        
        return data
    
    def _generate_smart_title(self, link: Link) -> str:
        """Generate smart title from URL or metadata"""
        # First try extracted metadata
        if link.metadata_ and 'page_metadata' in link.metadata_:
            metadata_title = link.metadata_['page_metadata'].get('title')
            if metadata_title:
                return metadata_title
        
        # Fallback to display URL
        return extract_display_url(link.original_url)
    
    def _serialize_short_link_data(self, link: Link) -> Dict[str, Any]:
        """Serialize short link specific data with enhanced features"""
        data = {
            'slug': link.slug,
            'short_url': get_short_link_url(link.slug) if link.slug else None,
            'click_count': link.click_count,
            'expires_at': link.expires_at.isoformat() if link.expires_at else None,
            'is_expired': link.expires_at and datetime.utcnow() > link.expires_at,
            'is_password_protected': bool(getattr(link, 'password_hash', None))
        }
        
        # Enhanced metadata from link metadata
        metadata = link.metadata_ or {}
        
        # Click limit information
        if metadata.get('click_limit'):
            data.update({
                'click_limit': metadata['click_limit'],
                'click_limit_reached': link.click_count >= metadata['click_limit'],
                'clicks_remaining': max(0, metadata['click_limit'] - link.click_count)
            })
        
        # UTM parameters
        if metadata.get('utm_params'):
            data['utm_params'] = metadata['utm_params']
        
        # Targeting information
        if metadata.get('geo_targeting'):
            data['geo_targeting'] = metadata['geo_targeting']
        
        if metadata.get('device_targeting'):
            data['device_targeting'] = metadata['device_targeting']
        
        # Performance metrics for short links
        if link.click_count > 0:
            # Calculate click rate (clicks per day since creation)
            if link.created_at:
                days_active = max(1, (datetime.utcnow() - link.created_at).days)
                data['daily_click_rate'] = round(link.click_count / days_active, 2)
            
            # Performance tier
            data['performance_tier'] = self._get_performance_tier(link.click_count)
        
        return data
    
    def _build_enhanced_preview_data(self, link: Link) -> Dict[str, Any]:
        """Build enhanced preview data with extracted metadata"""
        domain = extract_domain(link.original_url)
        
        # Get page metadata if available
        page_metadata = {}
        if link.metadata_ and 'page_metadata' in link.metadata_:
            page_metadata = link.metadata_['page_metadata']
        
        # Build base preview
        preview = {
            'domain': domain,
            'domain_info': self._get_enhanced_domain_info(domain)
        }
        
        # Enhanced favicon handling
        if page_metadata.get('favicon'):
            preview['favicon'] = page_metadata['favicon']
            preview['favicon_type'] = page_metadata.get('favicon_type', 'icon')
            preview['favicon_source'] = page_metadata.get('favicon_source', 'extracted')
        else:
            preview['favicon'] = build_favicon_url(link.original_url)
            preview['favicon_type'] = 'icon'
            preview['favicon_source'] = 'google'
        
        # Preview image with metadata
        if page_metadata.get('image'):
            preview.update({
                'image': page_metadata['image'],
                'has_preview_image': True,
                'image_type': 'extracted'
            })
        else:
            preview.update({
                'image': None,
                'has_preview_image': False,
                'image_type': None
            })
        
        # Rich metadata from extraction
        if page_metadata:
            preview.update({
                'site_name': page_metadata.get('site_name'),
                'author': page_metadata.get('author'),
                'published_time': page_metadata.get('published_time'),
                'theme_color': page_metadata.get('theme_color'),
                'content_type': page_metadata.get('type', 'website'),
                'language': page_metadata.get('language'),
                'keywords': page_metadata.get('keywords', [])[:5],  # Limit keywords
                'has_rich_metadata': page_metadata.get('has_rich_metadata', False),
                'extraction_success': page_metadata.get('extraction_success', False)
            })
            
            # Article-specific enhanced data
            if page_metadata.get('article_info'):
                article_info = page_metadata['article_info']
                preview['article_info'] = {
                    'reading_time': article_info.get('reading_time'),
                    'estimated_reading_minutes': article_info.get('estimated_reading_minutes'),
                    'word_count': article_info.get('estimated_word_count'),
                    'section': article_info.get('section')
                }
            
            # Video-specific data
            if page_metadata.get('video_info'):
                preview['video_info'] = page_metadata['video_info']
            
            # Product-specific data
            if page_metadata.get('product_info'):
                preview['product_info'] = page_metadata['product_info']
            
            # Social profiles
            if page_metadata.get('social_profiles'):
                preview['social_profiles'] = page_metadata['social_profiles'][:3]  # Limit to 3
        
        # Enhanced categorization
        preview['category'] = self._categorize_link_with_metadata(link, page_metadata)
        preview['category_confidence'] = self._get_categorization_confidence(link, page_metadata)
        
        # Content quality indicators
        preview['content_quality'] = self._assess_content_quality(link, page_metadata)
        
        return preview
    
    def _get_enhanced_domain_info(self, domain: str) -> Dict[str, Any]:
        """Get enhanced domain information with caching"""
        if not domain:
            return {}
        
        # Use cache to avoid repeated lookups
        if domain in self._domain_cache:
            return self._domain_cache[domain]
        
        # Enhanced domain categorization
        domain_info = {
            'is_social': domain in [
                'twitter.com', 'facebook.com', 'instagram.com', 'linkedin.com',
                'youtube.com', 'tiktok.com', 'reddit.com', 'discord.com',
                'snapchat.com', 'pinterest.com', 'tumblr.com'
            ],
            'is_dev': domain in [
                'github.com', 'gitlab.com', 'bitbucket.org', 'stackoverflow.com', 
                'developer.mozilla.org', 'docs.microsoft.com', 'nodejs.org', 
                'python.org', 'reactjs.org', 'vuejs.org', 'angular.io'
            ],
            'is_news': domain in [
                'cnn.com', 'bbc.com', 'reuters.com', 'bloomberg.com',
                'techcrunch.com', 'theverge.com', 'ycombinator.com',
                'nytimes.com', 'washingtonpost.com', 'guardian.com'
            ],
            'is_shopping': domain in [
                'amazon.com', 'ebay.com', 'etsy.com', 'shopify.com',
                'stripe.com', 'paypal.com', 'aliexpress.com', 'walmart.com'
            ],
            'is_media': domain in [
                'youtube.com', 'vimeo.com', 'twitch.tv', 'spotify.com',
                'soundcloud.com', 'netflix.com', 'imgur.com', 'flickr.com'
            ],
            'is_productivity': domain in [
                'notion.so', 'google.com', 'microsoft.com', 'dropbox.com',
                'slack.com', 'trello.com', 'asana.com', 'figma.com'
            ]
        }
        
        # Add trust indicators
        domain_info['trust_level'] = self._assess_domain_trust(domain)
        
        # Cache for future use
        self._domain_cache[domain] = domain_info
        return domain_info
    
    def _assess_domain_trust(self, domain: str) -> str:
        """Assess domain trustworthiness"""
        high_trust_domains = [
            'github.com', 'google.com', 'microsoft.com', 'mozilla.org',
            'w3.org', 'wikipedia.org', 'stackoverflow.com', 'medium.com'
        ]
        
        suspicious_patterns = [
            'bit.ly', 'tinyurl.com', 't.co', 'goo.gl'  # URL shorteners
        ]
        
        if domain in high_trust_domains:
            return 'high'
        elif any(pattern in domain for pattern in suspicious_patterns):
            return 'low'
        elif domain.count('.') > 2 or len(domain) > 30:
            return 'medium'
        else:
            return 'medium'
    
    def _categorize_link_with_metadata(self, link: Link, metadata: Dict[str, Any]) -> Optional[str]:
        """Enhanced categorization using extracted metadata"""
        # Use extracted type if available and specific
        extracted_type = metadata.get('type')
        if extracted_type and extracted_type not in ['website', 'object']:
            return extracted_type
        
        # Check structured data
        structured_data = metadata.get('structured_data', {})
        if 'VideoObject' in structured_data:
            return 'video'
        elif 'Article' in structured_data or 'BlogPosting' in structured_data:
            return 'article'
        elif 'Product' in structured_data:
            return 'product'
        elif 'Recipe' in structured_data:
            return 'recipe'
        elif 'Event' in structured_data:
            return 'event'
        elif 'SoftwareApplication' in structured_data:
            return 'app'
        
        # Enhanced keyword analysis
        keywords = metadata.get('keywords', [])
        keyword_str = ' '.join(keywords).lower()
        title = (link.title or '').lower()
        url = link.original_url.lower()
        combined_text = f"{keyword_str} {title} {url}"
        
        # More sophisticated categorization
        category_patterns = {
            'tutorial': ['tutorial', 'guide', 'howto', 'learn', 'course', 'lesson'],
            'documentation': ['docs', 'documentation', 'api', 'reference', 'manual'],
            'news': ['news', 'breaking', 'update', 'announcement', 'press'],
            'blog': ['blog', 'post', 'article', 'story', 'opinion'],
            'tool': ['tool', 'app', 'service', 'platform', 'utility', 'software'],
            'video': ['video', 'watch', 'youtube', 'vimeo', 'streaming'],
            'social': ['social', 'community', 'forum', 'discussion', 'chat'],
            'shopping': ['shop', 'buy', 'store', 'product', 'price', 'cart'],
            'research': ['research', 'study', 'paper', 'academic', 'journal'],
            'entertainment': ['game', 'fun', 'entertainment', 'movie', 'music']
        }
        
        for category, patterns in category_patterns.items():
            if any(pattern in combined_text for pattern in patterns):
                return category
        
        # Fallback to domain-based categorization
        domain = extract_domain(link.original_url)
        if domain:
            domain_info = self._get_enhanced_domain_info(domain)
            if domain_info.get('is_dev'):
                return 'development'
            elif domain_info.get('is_social'):
                return 'social'
            elif domain_info.get('is_news'):
                return 'news'
            elif domain_info.get('is_shopping'):
                return 'shopping'
            elif domain_info.get('is_media'):
                return 'media'
        
        return None
    
    def _get_categorization_confidence(self, link: Link, metadata: Dict[str, Any]) -> float:
        """Calculate confidence in categorization"""
        confidence = 0.0
        
        # High confidence from structured data
        if metadata.get('structured_data'):
            confidence += 0.8
        
        # Medium confidence from extracted metadata
        if metadata.get('type') and metadata['type'] != 'website':
            confidence += 0.6
        
        # Keywords provide some confidence
        if metadata.get('keywords'):
            confidence += 0.3
        
        # Title matching provides confidence
        if link.title:
            confidence += 0.2
        
        return min(1.0, confidence)
    
    def _assess_content_quality(self, link: Link, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Assess content quality indicators"""
        quality = {
            'score': 0,
            'indicators': []
        }
        
        # Rich metadata indicates quality
        if metadata.get('has_rich_metadata'):
            quality['score'] += 30
            quality['indicators'].append('Rich metadata')
        
        # Author information
        if metadata.get('author'):
            quality['score'] += 15
            quality['indicators'].append('Has author')
        
        # Publication date
        if metadata.get('published_time'):
            quality['score'] += 10
            quality['indicators'].append('Has publication date')
        
        # High-quality image
        if metadata.get('image'):
            quality['score'] += 20
            quality['indicators'].append('Has preview image')
        
        # Structured content
        if metadata.get('structured_data'):
            quality['score'] += 15
            quality['indicators'].append('Structured content')
        
        # Domain trust
        domain = extract_domain(link.original_url)
        if domain:
            trust_level = self._assess_domain_trust(domain)
            if trust_level == 'high':
                quality['score'] += 20
                quality['indicators'].append('Trusted domain')
        
        # Reading time indicates substantial content
        article_info = metadata.get('article_info', {})
        if article_info.get('estimated_reading_minutes', 0) > 2:
            quality['score'] += 10
            quality['indicators'].append('Substantial content')
        
        quality['score'] = min(100, quality['score'])
        return quality
    
    def _get_cached_link_tags(self, link: Link) -> List[Dict[str, Any]]:
        """Get link tags with caching"""
        cache_key = f"tags:{link.id}"
        
        if cache_key in self._tag_cache:
            return self._tag_cache[cache_key]
        
        try:
            tags = get_link_tags(link.id, link.user_id)
            tag_data = [{
                'id': tag.id,
                'name': tag.name,
                'color': tag.color
            } for tag in tags]
            
            self._tag_cache[cache_key] = tag_data
            return tag_data
            
        except Exception as e:
            logger.warning(f"Failed to get tags for link {link.id}: {e}")
            return []
    
    def _get_link_analytics_summary(self, link: Link) -> Optional[Dict[str, Any]]:
        """Get basic analytics summary for short links"""
        if not redis_client.available or link.link_type != 'shortened':
            return None
        
        try:
            analytics_key = f"shortlinks:{self.user_id}:analytics:{link.id}"
            analytics_json = redis_client.get(analytics_key)
            
            if analytics_json:
                analytics = json.loads(analytics_json)
                
                return {
                    'total_clicks': analytics.get('total_clicks', link.click_count),
                    'unique_clicks': analytics.get('unique_clicks', 0),
                    'top_country': self._get_top_item(analytics.get('countries', {})),
                    'top_device': self._get_top_item(analytics.get('devices', {})),
                    'top_referrer': self._get_top_item(analytics.get('referrers', {})),
                    'last_clicked': analytics.get('last_clicked'),
                    'click_rate': self._calculate_click_rate(link, analytics)
                }
        except Exception as e:
            logger.warning(f"Failed to get analytics for link {link.id}: {e}")
        
        return None
    
    def _calculate_click_rate(self, link: Link, analytics: Dict[str, Any]) -> Optional[float]:
        """Calculate click rate (clicks per day)"""
        if not link.created_at:
            return None
        
        days_active = max(1, (datetime.utcnow() - link.created_at).days)
        total_clicks = analytics.get('total_clicks', link.click_count)
        
        return round(total_clicks / days_active, 2)
    
    def _get_top_item(self, data: Dict[str, int]) -> Optional[str]:
        """Get the top item from a count dictionary"""
        if not data:
            return None
        return max(data.items(), key=lambda x: x[1])[0]
    
    def _get_performance_tier(self, click_count: int) -> str:
        """Get performance tier for short links"""
        if click_count >= 100:
            return 'high'
        elif click_count >= 10:
            return 'medium'
        elif click_count >= 1:
            return 'low'
        return 'none'
    
    def _build_smart_metadata(self, link: Link) -> Dict[str, Any]:
        """Build comprehensive smart metadata"""
        metadata = {
            'age_days': (datetime.utcnow() - link.created_at).days if link.created_at else None,
            'is_recent': link.created_at and (datetime.utcnow() - link.created_at).days <= 7,
            'has_activity': link.updated_at and link.updated_at > link.created_at + timedelta(seconds=1),
            'last_accessed': link.updated_at.isoformat() if link.updated_at else None
        }
        
        # Performance indicators for short links
        if link.link_type == 'shortened':
            metadata.update({
                'performance_tier': self._get_performance_tier(link.click_count),
                'engagement_score': self._calculate_engagement_score(link),
                'is_viral': link.click_count > 50,
                'click_velocity': self._calculate_click_velocity(link)
            })
        
        # Organization indicators
        metadata.update({
            'organization_score': self._calculate_organization_score(link),
            'has_metadata': bool(link.metadata_ and 'page_metadata' in link.metadata_),
            'completeness_score': self._calculate_completeness_score(link)
        })
        
        return metadata
    
    def _calculate_engagement_score(self, link: Link) -> float:
        """Calculate comprehensive engagement score"""
        score = 0.0
        
        # Click-based score
        if link.click_count > 0:
            score += min(50, link.click_count * 2)
        
        # Recency boost
        if link.updated_at:
            days_old = (datetime.utcnow() - link.updated_at).days
            if days_old <= 7:
                score += 20 * (1 - days_old / 7)
        
        # Status boosts
        if link.pinned:
            score += 20
        if getattr(link, 'starred', False):
            score += 15
        if getattr(link, 'frequently_used', False):
            score += 10
        
        # Organization boost
        if link.folder_id:
            score += 5
        
        return min(100.0, score)
    
    def _calculate_click_velocity(self, link: Link) -> Optional[float]:
        """Calculate click velocity (clicks per day since creation)"""
        if not link.created_at or link.click_count == 0:
            return 0.0
        
        days_since_creation = max(1, (datetime.utcnow() - link.created_at).days)
        return round(link.click_count / days_since_creation, 2)
    
    def _calculate_organization_score(self, link: Link) -> float:
        """Calculate how well organized a link is"""
        score = 0.0
        
        # Has meaningful title
        if link.title:
            score += 25
        
        # Has notes
        if link.notes:
            score += 20
        
        # In folder
        if link.folder_id:
            score += 30
        
        # Has tags
        try:
            tags = self._get_cached_link_tags(link)
            if tags:
                score += min(25, len(tags) * 8)
        except:
            pass
        
        return min(100.0, score)
    
    def _calculate_completeness_score(self, link: Link) -> float:
        """Calculate how complete the link information is"""
        score = 0.0
        
        # Basic information
        if link.title:
            score += 20
        if link.notes:
            score += 15
        
        # Metadata presence
        if link.metadata_:
            score += 20
            if link.metadata_.get('page_metadata'):
                page_meta = link.metadata_['page_metadata']
                if page_meta.get('description'):
                    score += 15
                if page_meta.get('image'):
                    score += 15
                if page_meta.get('author'):
                    score += 10
                if page_meta.get('keywords'):
                    score += 5
        
        return min(100.0, score)
    
    def _calculate_performance_metrics(self, link: Link) -> Dict[str, Any]:
        """Calculate comprehensive performance metrics"""
        metrics = {
            'engagement_score': self._calculate_engagement_score(link),
            'organization_score': self._calculate_organization_score(link),
            'completeness_score': self._calculate_completeness_score(link)
        }
        
        # Overall quality score (weighted average)
        weights = {
            'engagement_score': 0.4,
            'organization_score': 0.3,
            'completeness_score': 0.3
        }
        
        overall_score = sum(
            metrics[key] * weights[key] 
            for key in weights
        )
        
        metrics['overall_score'] = round(overall_score, 1)
        metrics['quality_tier'] = self._get_quality_tier(overall_score)
        
        return metrics
    
    def _get_quality_tier(self, score: float) -> str:
        """Get quality tier based on overall score"""
        if score >= 80:
            return 'excellent'
        elif score >= 60:
            return 'good'
        elif score >= 40:
            return 'average'
        else:
            return 'needs_improvement'

# Global serialization functions

def serialize_link(link: Link, include_analytics: bool = False, include_preview: bool = True) -> Dict[str, Any]:
    """Serialize single link with enhanced features"""
    serializer = EnhancedLinkSerializer(link.user_id)
    return serializer.serialize_link(link, include_analytics, include_preview)

def serialize_links(links: List[Link], include_analytics: bool = False) -> List[Dict[str, Any]]:
    """Serialize multiple links efficiently with shared serializer"""
    if not links:
        return []
    
    # Use same serializer instance for efficiency
    serializer = EnhancedLinkSerializer(links[0].user_id)
    return [
        serializer.serialize_link(link, include_analytics) 
        for link in links
    ]

def serialize_link_preview(link: Link) -> Dict[str, Any]:
    """Serialize link for preview/search results with minimal data"""
    serializer = EnhancedLinkSerializer(link.user_id)
    data = serializer.serialize_link(link, include_analytics=False, include_preview=True)
    
    # Return minimal preview data for performance
    return {
        'id': data['id'],
        'title': data['title'],
        'display_url': data['display_url'],
        'preview': data['preview'],
        'link_type': data['link_type'],
        'pinned': data['pinned'],
        'starred': data['starred'],
        'frequently_used': data['frequently_used'],
        'relative_time': data['relative_time'],
        'performance': data['performance']
    }

def serialize_links_minimal(links: List[Link]) -> List[Dict[str, Any]]:
    """Serialize links with minimal data for better performance"""
    return [serialize_link_preview(link) for link in links]

def get_serialization_stats(links: List[Link]) -> Dict[str, Any]:
    """Get statistics about link serialization"""
    if not links:
        return {}
    
    # Analyze links
    total_links = len(links)
    has_metadata = sum(1 for link in links if link.metadata_)
    has_rich_metadata = sum(
        1 for link in links 
        if link.metadata_ and link.metadata_.get('page_metadata', {}).get('has_rich_metadata')
    )
    
    short_links = sum(1 for link in links if link.link_type == 'shortened')
    saved_links = total_links - short_links
    
    return {
        'total_links': total_links,
        'saved_links': saved_links,
        'short_links': short_links,
        'has_metadata': has_metadata,
        'has_rich_metadata': has_rich_metadata,
        'metadata_coverage': (has_metadata / total_links * 100) if total_links > 0 else 0,
        'rich_metadata_coverage': (has_rich_metadata / total_links * 100) if total_links > 0 else 0
    }